{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['1', 'Australia AUS', '29', '4,837', '167']\n",
      "['2', 'England ENG', '33', '4,046', '123']\n",
      "['3', 'South Africa SA', '35', '4,157', '119']\n",
      "['4', 'India IND', '32', '3,219', '101']\n",
      "['5', 'New Zealand NZ', '31', '3,019', '97']\n",
      "['6', 'West Indies WI', '30', '2,768', '92']\n",
      "['7', 'Bangladesh BAN', '12', '930', '78']\n",
      "['8', 'Pakistan PAK', '30', '1,962', '65']\n",
      "['9', 'Ireland IRE', '11', '516', '47']\n",
      "['10', 'Sri Lanka SL', '11', '495', '45']\n",
      "['11', 'Zimbabwe ZIM', '8', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "# from settings import URL\n",
    "# Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "URL = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                parent = self.soup.find('ul', class_= 'listing cf')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                table = self.soup.find('table', {'class': 'table'})\n",
    "                for x in table:\n",
    "                        rows = table.find_all('tr')\n",
    "                for tr in rows:\n",
    "                        data_table = []\n",
    "                        cols = tr.find_all('td')\n",
    "                        for td in cols:\n",
    "                                data_table.append(td.text.strip())\n",
    "                                for i in range(len(data_table)):\n",
    "                                        data_table[i] = data_table[i].replace('\\n', ' ')\n",
    "                                # data_table_strip = '\\n'\n",
    "                        print(data_table)\n",
    "                # print(table)                 \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '(0)', 'Alyssa', 'Healy', 'AUS', '785']\n",
      "['2', '(0)', 'Beth', 'Mooney', 'AUS', '749']\n",
      "['3', '(0)', 'Natalie', 'Sciver', 'ENG', '747']\n",
      "['4', '(0)', 'Laura', 'Wolvaardt', 'SA', '732']\n",
      "['5', '(0)', 'Meg', 'Lanning', 'AUS', '710']\n",
      "['6', '(0)', 'Rachael', 'Haynes', 'AUS', '701']\n",
      "['7', '(0)', 'Amy', 'Satterthwaite', 'NZ', '681']\n",
      "['8', '(0)', 'Tammy', 'Beaumont', 'ENG', '667']\n",
      "['9', '(0)', 'Chamari', 'Athapaththu', 'SL', '655']\n",
      "['10', '(0)', 'Smriti', 'Mandhana', 'IND', '649']\n"
     ]
    }
   ],
   "source": [
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from tkinter.ttk import Separator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Top 10 women’s ODI Batting players along with the records of their team and rating\n",
    "URL = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                # parent = self.soup.find('div', class_= 'col-4 col-12-desk touch-scroll-list__element')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                \n",
    "                table_data = self.soup.find(\"div\", attrs= {\"data-cricket-role\":\"batting\"}).find(\"div\", class_ = \"rankings-block__top-player\").get_text(strip=True, separator=\" \").split(\" \")\n",
    "                other_data = self.soup.find(\"div\", attrs={\"data-cricket-role\":\"batting\"}).find_all(\"tr\", class_ = \"table-body\")\n",
    "                \n",
    "                list_player = []\n",
    "                list_player.append(table_data)\n",
    "                \n",
    "                for i in other_data:\n",
    "                        \n",
    "                        split_list = i.get_text(strip = True, separator = \" \").split(\" \")\n",
    "                        list_player.append(split_list)\n",
    "                print(*list_player, sep= \"\\n\")\n",
    "                        \n",
    "                # for x in table:\n",
    "                #         rows = table.find_all('div')\n",
    "                #         for div in rows:\n",
    "                #                 odi_batsman = []\n",
    "                #                 col = div.find_all('div')\n",
    "                #                 # for div in col:\n",
    "                #                 #         odi_batsman.append(div.get_attribute())\n",
    "                #         odi_batsman.append(rows)\n",
    "                #         print(odi_batsman)\n",
    "                        \n",
    "                # for tr in rows:\n",
    "                #         data_table = []\n",
    "                #         cols = tr.find_all('td')\n",
    "                #         for td in cols:\n",
    "                #                 data_table.append(td.text.strip())\n",
    "                #                 for i in range(len(data_table)):\n",
    "                #                         data_table[i] = data_table[i].replace('\\n', ' ')\n",
    "                #                 # data_table_strip = '\\n'\n",
    "                #         print(data_table)\n",
    "                # print(table)                 \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natalie', 'Sciver', 'ENG', '379']\n",
      "['2', '(0)', 'Ellyse', 'Perry', 'AUS', '374']\n",
      "['3', '(0)', 'Marizanne', 'Kapp', 'SA', '349']\n",
      "['4', '(0)', 'Hayley', 'Matthews', 'WI', '339']\n",
      "['5', '(0)', 'Amelia', 'Kerr', 'NZ', '336']\n",
      "['6', '(0)', 'Ashleigh', 'Gardner', 'AUS', '270']\n",
      "['7', '(0)', 'Deepti', 'Sharma', 'IND', '252']\n",
      "['8', '(0)', 'Jess', 'Jonassen', 'AUS', '246']\n",
      "['9', '(0)', 'Katherine', 'Brunt', 'ENG', '220']\n",
      "['10', '(0)', 'Stafanie', 'Taylor', 'WI', '207']\n"
     ]
    }
   ],
   "source": [
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from tkinter.ttk import Separator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Top 10 women’s ODI all rounder players along with the records of their team and rating\n",
    "URL = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                # parent = self.soup.find('div', class_= 'col-4 col-12-desk touch-scroll-list__element')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                \n",
    "                table_data = self.soup.find(\"div\", attrs= {\"data-cricket-role\": \"all_round\"}).find(\"div\", class_ = \"rankings-block__banner--player-info\").get_text(strip=True, separator=\" \").split(\" \")\n",
    "                other_data = self.soup.find(\"div\", attrs={\"data-cricket-role\": \"all_round\"}).find_all(\"tr\", class_ = \"table-body\")\n",
    "                \n",
    "                list_player = []\n",
    "                list_player.append(table_data)\n",
    "                \n",
    "                for i in other_data:\n",
    "                        \n",
    "                        split_list = i.get_text(strip = True, separator = \" \").split(\" \")\n",
    "                        list_player.append(split_list)\n",
    "                print(*list_player, sep= \"\\n\")\n",
    "                        \n",
    "              \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n",
    "\n",
    "\n",
    "# Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8d58dd5be0ada4fb5a47414fe14ff43adb412c7f275b2cb4b408d3aefadbe44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

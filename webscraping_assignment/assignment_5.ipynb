{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['1', 'New Zealand NZ', '19', '2,355', '124']\n",
      "['2', 'England ENG', '27', '3,226', '119']\n",
      "['3', 'India IND', '31', '3,447', '111']\n",
      "['4', 'Pakistan PAK', '22', '2,354', '107']\n",
      "['5', 'Australia AUS', '25', '2,548', '102']\n",
      "['6', 'South Africa SA', '21', '2,111', '101']\n",
      "['7', 'Bangladesh BAN', '30', '2,753', '92']\n",
      "['8', 'Sri Lanka SL', '29', '2,658', '92']\n",
      "['9', 'West Indies WI', '41', '2,902', '71']\n",
      "['10', 'Afghanistan AFG', '18', '1,238', '69']\n",
      "['11', 'Ireland IRE', '23', '1,214', '53']\n",
      "['12', 'Scotland SCO', '27', '1,254', '46']\n",
      "['13', 'Zimbabwe ZIM', '25', '973', '39']\n",
      "['14', 'Netherlands NED', '21', '673', '32']\n",
      "['15', 'UAE UAE', '22', '697', '32']\n",
      "['16', 'United States USA', '23', '725', '32']\n",
      "['17', 'Oman OMA', '30', '919', '31']\n",
      "['18', 'Namibia NAM', '15', '369', '25']\n",
      "['19', 'Nepal NEP', '22', '331', '15']\n",
      "['20', 'Papua New Guinea PNG', '22', '134', '6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "# from settings import URL\n",
    "# Top 10 ODI teams in menâ€™s cricket along with the records for matches, points and rating.\n",
    "URL = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                parent = self.soup.find('ul', class_= 'listing cf')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                table = self.soup.find('table', {'class': 'table'})\n",
    "                for x in table:\n",
    "                        rows = table.find_all('tr')\n",
    "                for tr in rows:\n",
    "                        data_table = []\n",
    "                        cols = tr.find_all('td')\n",
    "                        for td in cols:\n",
    "                                data_table.append(td.text.strip())\n",
    "                                for i in range(len(data_table)):\n",
    "                                        data_table[i] = data_table[i].replace('\\n', ' ')\n",
    "                                # data_table_strip = '\\n'\n",
    "                        print(data_table)\n",
    "                # print(table)                 \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '(0)', 'Babar', 'Azam', 'PAK', '890']\n",
      "['2', '(0)', 'Rassie', 'van', 'der', 'Dussen', 'SA', '789']\n",
      "['3', '(0)', 'Quinton', 'de', 'Kock', 'SA', '784']\n",
      "['4', '(0)', 'Imam-ul-Haq', 'PAK', '779']\n",
      "['5', '(0)', 'Virat', 'Kohli', 'IND', '744']\n",
      "['6', '(0)', 'Rohit', 'Sharma', 'IND', '740']\n",
      "['7', '(0)', 'David', 'Warner', 'AUS', '739']\n",
      "['8', '(0)', 'Jonny', 'Bairstow', 'ENG', '732']\n",
      "['9', '(0)', 'Ross', 'Taylor', 'NZ', '722']\n",
      "['10', '(0)', 'Aaron', 'Finch', 'AUS', '706']\n"
     ]
    }
   ],
   "source": [
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from tkinter.ttk import Separator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                # parent = self.soup.find('div', class_= 'col-4 col-12-desk touch-scroll-list__element')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                \n",
    "                table_data = self.soup.find(\"div\", attrs= {\"data-cricket-scope\": \"odi\"}).find(\"div\", class_ = \"rankings-block__top-player\").get_text(strip=True, separator=\" \").split(\" \")\n",
    "                other_data = self.soup.find(\"div\", attrs={\"data-cricket-scope\": \"odi\"}).find_all(\"tr\", class_ = \"table-body\")\n",
    "                \n",
    "                list_player = []\n",
    "                list_player.append(table_data)\n",
    "                \n",
    "                for i in other_data:\n",
    "                        \n",
    "                        split_list = i.get_text(strip = True, separator = \" \").split(\" \")\n",
    "                        list_player.append(split_list)\n",
    "                print(*list_player, sep= \"\\n\")\n",
    "                        \n",
    "                # for x in table:\n",
    "                #         rows = table.find_all('div')\n",
    "                #         for div in rows:\n",
    "                #                 odi_batsman = []\n",
    "                #                 col = div.find_all('div')\n",
    "                #                 # for div in col:\n",
    "                #                 #         odi_batsman.append(div.get_attribute())\n",
    "                #         odi_batsman.append(rows)\n",
    "                #         print(odi_batsman)\n",
    "                        \n",
    "                # for tr in rows:\n",
    "                #         data_table = []\n",
    "                #         cols = tr.find_all('td')\n",
    "                #         for td in cols:\n",
    "                #                 data_table.append(td.text.strip())\n",
    "                #                 for i in range(len(data_table)):\n",
    "                #                         data_table[i] = data_table[i].replace('\\n', ' ')\n",
    "                #                 # data_table_strip = '\\n'\n",
    "                #         print(data_table)\n",
    "                # print(table)                 \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '(0)', 'Trent', 'Boult', 'NZ', '720']\n",
      "['2', '(0)', 'Josh', 'Hazlewood', 'AUS', '678']\n",
      "['3', '(0)', 'Mujeeb', 'Ur', 'Rahman', 'AFG', '676']\n",
      "['4', '(0)', 'Jasprit', 'Bumrah', 'IND', '662']\n",
      "['5', '(0)', 'Shaheen', 'Afridi', 'PAK', '661']\n",
      "['6', '(0)', 'Mohammad', 'Nabi', 'AFG', '657']\n",
      "['7', '(0)', 'Mehedi', 'Hasan', 'BAN', '655']\n",
      "['8', '(0)', 'Rashid', 'Khan', 'AFG', '651']\n",
      "['9', '(0)', 'Matt', 'Henry', 'NZ', '644']\n",
      "['10', '(0)', 'Mustafizur', 'Rahman', 'BAN', '640']\n"
     ]
    }
   ],
   "source": [
    "from gettext import install\n",
    "from inspect import Attribute\n",
    "from operator import index\n",
    "from tkinter.ttk import Separator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import urllib.request\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Top 10 ODI Bowler along with the records of their team and rating.\n",
    "URL = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "import uuid\n",
    "uuid4 = uuid.uuid4()\n",
    "class scraper:\n",
    "        def __init__(self) -> None:\n",
    "                request = requests.get(URL)\n",
    "                htmlContent = request.content\n",
    "                self.soup =bs(htmlContent, \"html.parser\")\n",
    "                \n",
    "                \n",
    "        def nevigate_page(self):\n",
    "                parent = self.soup.find('div', class_= 'col-4 col-12-desk touch-scroll-list__element')\n",
    "                # child = parent.find('h3')\n",
    "                # print(parent)\n",
    "                \n",
    "                table_data = self.soup.find(\"div\", attrs= {\"data-cricket-role\": \"bowling\"}).find(\"div\", class_ = \"rankings-block__top-player\").get_text(strip=True, separator = \" \").split(\" \")\n",
    "                other_data = self.soup.find(\"div\", attrs={\"data-cricket-role\": \"bowling\"}).find_all(\"tr\", class_ = \"table-body\")\n",
    "                \n",
    "                list_player = []\n",
    "                list_player.append(table_data)\n",
    "                \n",
    "                for i in other_data:\n",
    "                        \n",
    "                        split_list = i.get_text(strip = True, separator = \" \").split(\" \")\n",
    "                        list_player.append(split_list)\n",
    "                print(*list_player, sep= \"\\n\")\n",
    "                # print(other_data)\n",
    "                        \n",
    "                # for x in table:\n",
    "                #         rows = table.find_all('div')\n",
    "                #         for div in rows:\n",
    "                #                 odi_batsman = []\n",
    "                #                 col = div.find_all('div')\n",
    "                #                 # for div in col:\n",
    "                #                 #         odi_batsman.append(div.get_attribute())\n",
    "                #         odi_batsman.append(rows)\n",
    "                #         print(odi_batsman)\n",
    "                        \n",
    "                # for tr in rows:\n",
    "                #         data_table = []\n",
    "                #         cols = tr.find_all('td')\n",
    "                #         for td in cols:\n",
    "                #                 data_table.append(td.text.strip())\n",
    "                #                 for i in range(len(data_table)):\n",
    "                #                         data_table[i] = data_table[i].replace('\\n', ' ')\n",
    "                #                 # data_table_strip = '\\n'\n",
    "                #         print(data_table)\n",
    "                # print(table)                 \n",
    "                        \n",
    "        def main(self) -> None:\n",
    "                self.nevigate_page()\n",
    "                # self.get_details()\n",
    "                #self.page_scroll()\n",
    "                            \n",
    "if __name__ == \"__main__\":\n",
    "    DPS = scraper()\n",
    "    DPS.main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8d58dd5be0ada4fb5a47414fe14ff43adb412c7f275b2cb4b408d3aefadbe44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
